# How to Beta
When you're ready for your project to be launched to the wider community, you'll hit the "Apply for Review" button. This sends and email to the Zooniverse team, who review your project and decide whether or not it's ready to be sent out to the community for a full on review and beta test. If your project is approved for review, then it will be sent out to our beta testing community, who will make classifications and provide suggestions for improvement. After you address reviewer feedback and ensure that your classification data make sense, you can apply for a full Zooniverse launch.

## The goals of the review are:
1. Ensure that the project is suitable and ready for a full, public launch to the Zooniverse homepage.
2. Get feedback from the Zooniverse community on how to improve the project prior to a full launch. Depending on the state of your project, feedback can vary from minor text changes to major interface redesign.
3. Acquire enough data to ensure that your project can produce meaningful results.

To make the most of this review, you want your project to be as near to final as possible. We have many projects going for review, and need to consider our community's limited time, so we recommend you take advantage of the project builder boards when initially designing your project.

## Beta Data: Preparing a beta dataset
Beta testing is an opportunity to ensure that your project can yield _useful_ data that makes sense, as well as an opportunity to assess how many classifications you need to confidently retire an image. To get the most out of your beta test, we recommend the following:
1. Select 50-100 _randomly sampled images_ from your overall dataset. It is important that these be randomly sampled so that you can assess overall accuracy.
2. Link _only_ this subject set to your workflow(s) so that classifiers only classify this specific subject set.
3. Set retirement limits _high_ - 20 or higher. You want images to be "overclassified" - to have more classifications than they really need.
4. After running beta, download your **aggregated data** from the Data Exports page and review it.  

## Data Validation (not really ready yet)
We are still developing tools to make creating and comparing gold standard data easier, but you _can_ also contribute gold standard classifications during beta testing. Any project owner, collaborator, or expert should see a gear icon next to the "Next" button. You can use this gear icon to identify your classification as "gold standard."

Right now there is no easy way to compare gold standard classifications to aggregated data, but eventually we hope to provide tools for such comparison. We also eventually hope to provide tools to assess changes in accuracy of aggregation for varying numbers of classifiers (e.g. how much does accuracy change with 3, 5, 10 classifiers?)
